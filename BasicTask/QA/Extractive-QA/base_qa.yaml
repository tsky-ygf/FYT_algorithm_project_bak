LogArguments:
  is_debug: True
  log_level: "DEBUG"

TrainingArguments:
  train_batch_size: 32
  eval_batch_size: 4
  lr_scheduler_type: "linear"
  learning_rate: 0.00005
  num_train_epochs: 20
  accelerator_params:
    fp16: True

ModelArguments:
  model_name_or_path: "model/language_model/lawformer"
  tokenizer_name: "model/language_model/chinese-roberta-wwm-ext/"
  max_length: 512

DataTrainingArguments:
  train_data_path: "data/fyt_train_use_data/CAIL-Long/civil/train.json"
  dev_data_path: "data/fyt_train_use_data/CAIL-Long/civil/dev.json"
  test_data_path: "data/fyt_train_use_data/CAIL-Long/civil/test.json"
  output_dir: "model/train_result/Extractive-QA/"