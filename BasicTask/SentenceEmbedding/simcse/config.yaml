#log_level:
is_debug: False
log_level: "INFO"
log_path: "log/tensorboard_log/simcse"

#training_args:
train_batch_size: 24
eval_batch_size: 24
learning_rate: 0.00002
num_train_epochs: 8
early_stop_patience: 3

lr_scheduler_type: "cosine" # "linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"
warmup_ratio: 0.1
#num_warmup_steps: 100
gradient_accumulation_steps: 1
max_grad_norm: 1.0
weight_decay: 0.01
eval_every_number_of_epoch: 1

accelerator_params:
  fp16: True
do_adv: False
adv_name: "word_embeddings"
adv_epsilon: 1.0

# data args:
preprocessing_num_workers: null
pad_to_max_length: False
overwrite_cache: False

#model args:
#num_labels: 11
max_length: 128
dropout: 0.1
cache_dir: null
pooler_type: "cls"
mlm_weight: 0.1
temp: 0.05

#tokenizer args:
use_fast_tokenizer: True
model_revision: main
use_auth_token: False


do_mlm: False

pre_train_tokenizer: "model/language_model/bert-base-chinese"
pre_train_model: "model/language_model/bert-base-chinese"

# output model path
output_dir: "model/tmp/nizhu"