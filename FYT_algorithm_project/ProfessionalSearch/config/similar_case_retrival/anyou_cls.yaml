# log level
is_debug: False
log_level: "INFO"

# training args
train_batch_size: 32
eval_batch_size: 4
lr_scheduler_type: "linear"
num_warmup_steps: 0
gradient_accumulation_steps: 1
weight_decay: 0.01
learning_rate: 0.00005
num_train_epochs: 20
eval_every_number_of_epoch: 1
early_stop_patience: 5

# model args
num_labels: 4
#max_len: 512
#dropout: 0.1
#feature_dim: 128

pre_train_tokenizer: "model/language_model/chinese-roberta-wwm-ext/"
pre_train_model: "model/language_model/lawformer"

train_data_path: ""
dev_data_path: ""
test_data_path: ""

# output model path
output_dir: "model/AnyouCls/"